{"Structs/Trace/Level.html#/s:10Spokestack5TraceV5LevelO4NONEyA2EmF":{"name":"NONE","abstract":"<p>No traces</p>","parent_name":"Level"},"Structs/Trace/Level.html#/s:10Spokestack5TraceV5LevelO4INFOyA2EmF":{"name":"INFO","abstract":"<p>Informational traces</p>","parent_name":"Level"},"Structs/Trace/Level.html#/s:10Spokestack5TraceV5LevelO4PERFyA2EmF":{"name":"PERF","abstract":"<p>Performance traces</p>","parent_name":"Level"},"Structs/Trace/Level.html#/s:10Spokestack5TraceV5LevelO5DEBUGyA2EmF":{"name":"DEBUG","abstract":"<p>All the traces</p>","parent_name":"Level"},"Structs/Trace/Level.html":{"name":"Level","abstract":"<p>Undocumented</p>","parent_name":"Trace"},"Structs/Trace.html#/s:10Spokestack5TraceV5trace_7message6config7context6calleryAC5LevelO_SSAA19SpeechConfigurationCSgAA0I7ContextCSgyptFZ":{"name":"trace(_:message:config:context:caller:)","abstract":"<p>Traces a  message  from a Spokestack module.</p>","parent_name":"Trace"},"Structs/Trace.html#/s:10Spokestack5TraceV5trace_7message6config9delegates6calleryAC5LevelO_SSAA19SpeechConfigurationCSayAA6Tracer_pGyptFZ":{"name":"trace(_:message:config:delegates:caller:)","abstract":"<p>Traces a message from a Spokestack module.</p>","parent_name":"Trace"},"Structs/Trace.html#/s:10Spokestack5TraceV4spit4data8fileName7context6configy10Foundation4DataV_SSAA13SpeechContextCSgAA0K13ConfigurationCSgtFZ":{"name":"spit(data:fileName:context:config:)","abstract":"<p>Write data to a file, after clojure/core&rsquo;s <code>spit</code>.</p>","parent_name":"Trace"},"Structs/SignalProcessing/FFTWindowType.html#/s:10Spokestack16SignalProcessingV13FFTWindowTypeO4hannyA2EmF":{"name":"hann","abstract":"<p>Undocumented</p>","parent_name":"FFTWindowType"},"Structs/SignalProcessing.html#/s:10Spokestack16SignalProcessingV3rmsySf10Foundation4DataV_Says5Int16VGtFZ":{"name":"rms(_:_:)","abstract":"<p>Find the root mean squared of a frame buffer of samples.</p>","parent_name":"SignalProcessing"},"Structs/SignalProcessing/FFTWindowType.html":{"name":"FFTWindowType","abstract":"<p>Convenience enum for Fast Fourier Transform window types.</p>","parent_name":"SignalProcessing"},"Structs/SignalProcessing.html#/s:10Spokestack16SignalProcessingV17fftWindowDispatch10windowType0G6LengthSaySfGAC09FFTWindowH0O_SitFZ":{"name":"fftWindowDispatch(windowType:windowLength:)","abstract":"<p>Convenience function to find the window of a FFT.</p>","parent_name":"SignalProcessing"},"Structs/SignalProcessing.html#/s:10Spokestack16SignalProcessingV10hannWindowySaySfGSiFZ":{"name":"hannWindow(_:)","abstract":"<p>Implementation of the Hann smoothing function algorithm.</p>","parent_name":"SignalProcessing"},"Structs/SignalProcessing.html":{"name":"SignalProcessing","abstract":"<p>Static namepsace for signal processing functions.</p>"},"Structs/Trace.html":{"name":"Trace","abstract":"<p>Debugging trace levels, for simple filtering.</p>"},"Protocols/TranscriptEditor.html#/c:@M@Spokestack@objc(pl)TranscriptEditor(im)editTranscriptWithTranscript:":{"name":"editTranscript(transcript:)","abstract":"<p>Edit the ASR transcript to correct errors or perform other normalization before NLU classification occurs.</p>","parent_name":"TranscriptEditor"},"Protocols/Tracer.html#/c:@M@Spokestack@objc(pl)Tracer(im)didTrace:":{"name":"didTrace(_:)","abstract":"<p>The trace event.</p>","parent_name":"Tracer"},"Protocols/SpokestackDelegate.html#/c:@M@Spokestack@objc(pl)SpokestackDelegate(im)didInit":{"name":"didInit()","abstract":"<p>The speech pipeline has been initialized.</p>","parent_name":"SpokestackDelegate"},"Protocols/SpokestackDelegate.html#/c:@M@Spokestack@objc(pl)SpokestackDelegate(im)didStart":{"name":"didStart()","abstract":"<p>The speech pipeline has been started.</p>","parent_name":"SpokestackDelegate"},"Protocols/SpokestackDelegate.html#/c:@M@Spokestack@objc(pl)SpokestackDelegate(im)didStop":{"name":"didStop()","abstract":"<p>The speech pipeline has been stopped.</p>","parent_name":"SpokestackDelegate"},"Protocols/SpokestackDelegate.html#/c:@M@Spokestack@objc(pl)SpokestackDelegate(im)didActivate":{"name":"didActivate()","abstract":"<p>The pipeline activate event. Occurs upon activation of speech recognition.  The pipeline remains active until the user stops talking or the activation timeout is reached.</p>","parent_name":"SpokestackDelegate"},"Protocols/SpokestackDelegate.html#/c:@M@Spokestack@objc(pl)SpokestackDelegate(im)didDeactivate":{"name":"didDeactivate()","abstract":"<p>The pipeline deactivate event. Occurs upon deactivation of speech recognition.  The pipeline remains inactive until activated again by either explicit activation or wakeword activation.</p>","parent_name":"SpokestackDelegate"},"Protocols/SpokestackDelegate.html#/c:@M@Spokestack@objc(pl)SpokestackDelegate(im)didRecognize:":{"name":"didRecognize(_:)","abstract":"<p>The pipeline recognized and transcribed speech.</p>","parent_name":"SpokestackDelegate"},"Protocols/SpokestackDelegate.html#/c:@M@Spokestack@objc(pl)SpokestackDelegate(im)didRecognizePartial:":{"name":"didRecognizePartial(_:)","abstract":"<p>The pipeline recognized and transcribed a portion of an incomplete utterance.</p>","parent_name":"SpokestackDelegate"},"Protocols/SpokestackDelegate.html#/c:@M@Spokestack@objc(pl)SpokestackDelegate(im)didTimeout":{"name":"didTimeout()","abstract":"<p>The pipeline timeout event. The pipeline experienced a timeout in a component.</p>","parent_name":"SpokestackDelegate"},"Protocols/SpokestackDelegate.html#/c:@M@Spokestack@objc(pl)SpokestackDelegate(im)successWithResult:":{"name":"success(result:)","abstract":"<p>The TTS synthesis request has resulted in a successful response.</p>","parent_name":"SpokestackDelegate"},"Protocols/SpokestackDelegate.html#/c:@M@Spokestack@objc(pl)SpokestackDelegate(im)didBeginSpeaking":{"name":"didBeginSpeaking()","abstract":"<p>The TTS synthesis request has begun playback over the default audio system.</p>","parent_name":"SpokestackDelegate"},"Protocols/SpokestackDelegate.html#/c:@M@Spokestack@objc(pl)SpokestackDelegate(im)didFinishSpeaking":{"name":"didFinishSpeaking()","abstract":"<p>The TTS synthesis request has finished playback.</p>","parent_name":"SpokestackDelegate"},"Protocols/SpokestackDelegate.html#/c:@M@Spokestack@objc(pl)SpokestackDelegate(im)classificationWithResult:":{"name":"classification(result:)","abstract":"<p>The NLU classifier has produced a result.</p>","parent_name":"SpokestackDelegate"},"Protocols/SpokestackDelegate.html#/c:@M@Spokestack@objc(pl)SpokestackDelegate(im)didTrace:":{"name":"didTrace(_:)","abstract":"<p>The trace event.</p>","parent_name":"SpokestackDelegate"},"Protocols/SpokestackDelegate.html#/c:@M@Spokestack@objc(pl)SpokestackDelegate(im)failureWithError:":{"name":"failure(error:)","abstract":"<p>The error event. An error occured in a Spokestack module.</p>","parent_name":"SpokestackDelegate"},"Protocols/SpeechProcessor.html#/c:@M@Spokestack@objc(pl)SpeechProcessor(py)configuration":{"name":"configuration","abstract":"<p>The global configuration for all speech pipeline components.</p>","parent_name":"SpeechProcessor"},"Protocols/SpeechProcessor.html#/c:@M@Spokestack@objc(pl)SpeechProcessor(py)context":{"name":"context","abstract":"<p>Global speech context.</p>","parent_name":"SpeechProcessor"},"Protocols/SpeechProcessor.html#/c:@M@Spokestack@objc(pl)SpeechProcessor(im)startStreaming":{"name":"startStreaming()","abstract":"<p>Trigger from the speech pipeline for the component to begin processing the audio stream.</p>","parent_name":"SpeechProcessor"},"Protocols/SpeechProcessor.html#/c:@M@Spokestack@objc(pl)SpeechProcessor(im)stopStreaming":{"name":"stopStreaming()","abstract":"<p>Trigger from the speech pipeline for the component to stop processing the audio stream.</p>","parent_name":"SpeechProcessor"},"Protocols/SpeechProcessor.html#/c:@M@Spokestack@objc(pl)SpeechProcessor(im)process:":{"name":"process(_:)","abstract":"<p>Receives a frame of audio samples for processing. Interface between the <code>SpeechProcessor</code> and <code>AudioController</code> components.</p>","parent_name":"SpeechProcessor"},"Protocols/NLUService.html#/c:@M@Spokestack@objc(pl)NLUService(py)configuration":{"name":"configuration","abstract":"<p>The global configuration for all speech pipeline components.</p>","parent_name":"NLUService"},"Protocols/NLUService.html#/c:@M@Spokestack@objc(pl)NLUService(py)delegates":{"name":"delegates","abstract":"<p>Delegate that receives NLU service events.</p>","parent_name":"NLUService"},"Protocols/NLUService.html#/c:@M@Spokestack@objc(pl)NLUService(im)init:configuration:error:":{"name":"init(_:configuration:)","abstract":"<p>The initializer for the NLU service.</p>","parent_name":"NLUService"},"Protocols/NLUService.html#/c:@M@Spokestack@objc(pl)NLUService(im)classifyWithUtterance:context:":{"name":"classify(utterance:context:)","abstract":"<p>Classifies a user utterance into an intent, sending the result to the NLUDelegate.</p>","parent_name":"NLUService"},"Protocols/NLUService.html":{"name":"NLUService","abstract":"<p>A simple protocol for NLU services that provide intent classification and slot recognition, either on-device or via a network request.</p>"},"Protocols/SpeechProcessor.html":{"name":"SpeechProcessor","abstract":"<p>Protocol for speech pipeline components to receive speech pipeline coordination events.</p>"},"Protocols/SpokestackDelegate.html":{"name":"SpokestackDelegate","abstract":"<p>Protocol for receiving events from Spokestack modules for speech, nlu, and tts.</p>"},"Protocols/Tracer.html":{"name":"Tracer","abstract":"<p>Undocumented</p>"},"Protocols/TranscriptEditor.html":{"name":"TranscriptEditor","abstract":"<p>A functional interface used to edit an ASR transcript before it is passed to the NLU module for classification.</p>"},"Extensions/Array.html#/s:Sa10SpokestackSfRszlE6argmaxSi_SftyF":{"name":"argmax()","abstract":"<p>Returns the index and value of the largest number in the array.</p>","parent_name":"Array"},"Extensions/Array.html#/s:Sa10SpokestackSo8NSObjectCmRszlE18areSameOrderedType5otherSbSayypG_tF":{"name":"areSameOrderedType(other:)","abstract":"<p>Assert that each element, in order, of this array and the other array are the same type</p>","parent_name":"Array"},"Extensions/Array.html":{"name":"Array"},"Enums/VADMode.html#/c:@M@Spokestack@E@VADMode@VADModeHighlyPermissive":{"name":"HighlyPermissive","abstract":"<p>Most permissive of non-speech; most likely to detect speech.</p>","parent_name":"VADMode"},"Enums/VADMode.html#/c:@M@Spokestack@E@VADMode@VADModePermissive":{"name":"Permissive","abstract":"<p>Allows more non-speech than higher levels.</p>","parent_name":"VADMode"},"Enums/VADMode.html#/c:@M@Spokestack@E@VADMode@VADModeRestrictive":{"name":"Restrictive","abstract":"<p>Allows less non-speech than higher levels.</p>","parent_name":"VADMode"},"Enums/VADMode.html#/c:@M@Spokestack@E@VADMode@VADModeHighlyRestrictive":{"name":"HighlyRestrictive","abstract":"<p>Most restrictive of non-speech; most amount of missed speech.</p>","parent_name":"VADMode"},"Enums/TTSInputFormat.html#/c:@M@Spokestack@E@TTSInputFormat@TTSInputFormatText":{"name":"text","abstract":"<p>Plain text</p>","parent_name":"TTSInputFormat"},"Enums/TTSInputFormat.html#/c:@M@Spokestack@E@TTSInputFormat@TTSInputFormatSsml":{"name":"ssml","abstract":"<p><a href=\"https://spokestack.io/docs/Concepts/tts\">Speech Synthesis Markup Language</a></p>","parent_name":"TTSInputFormat"},"Enums/TTSInputFormat.html#/c:@M@Spokestack@E@TTSInputFormat@TTSInputFormatMarkdown":{"name":"markdown","abstract":"<p><a href=\"https://www.speechmarkdown.org/\">SpeechMarkdown</a></p>","parent_name":"TTSInputFormat"},"Enums/SpeechPipelineProfiles.html#/c:@M@Spokestack@E@SpeechPipelineProfiles@SpeechPipelineProfilesTfLiteWakewordAppleSpeech":{"name":"tfLiteWakewordAppleSpeech","abstract":"<p>VAD-sensitive TFLiteWakeword activates Apple ASR</p>","parent_name":"SpeechPipelineProfiles"},"Enums/SpeechPipelineProfiles.html#/c:@M@Spokestack@E@SpeechPipelineProfiles@SpeechPipelineProfilesVadTriggerAppleSpeech":{"name":"vadTriggerAppleSpeech","abstract":"<p>VAD-triggered Apple ASR</p>","parent_name":"SpeechPipelineProfiles"},"Enums/SpeechPipelineProfiles.html#/c:@M@Spokestack@E@SpeechPipelineProfiles@SpeechPipelineProfilesPushToTalkAppleSpeech":{"name":"pushToTalkAppleSpeech","abstract":"<p>Apple ASR that is manually activated and deactivated</p>","parent_name":"SpeechPipelineProfiles"},"Enums/SpeechPipelineProfiles.html#/c:@M@Spokestack@E@SpeechPipelineProfiles@SpeechPipelineProfilesTfLiteWakewordSpokestackSpeech":{"name":"tfLiteWakewordSpokestackSpeech","abstract":"<p>VAD-sensitive TFLiteWakeword activates Spokestack ASR</p>","parent_name":"SpeechPipelineProfiles"},"Enums/SpeechPipelineProfiles.html#/c:@M@Spokestack@E@SpeechPipelineProfiles@SpeechPipelineProfilesVadTriggerSpokestackSpeech":{"name":"vadTriggerSpokestackSpeech","abstract":"<p>VAD-triggered Spokestack ASR</p>","parent_name":"SpeechPipelineProfiles"},"Enums/SpeechPipelineProfiles.html#/c:@M@Spokestack@E@SpeechPipelineProfiles@SpeechPipelineProfilesPushToTalkSpokestackSpeech":{"name":"pushToTalkSpokestackSpeech","abstract":"<p>Spokestack ASR that is manually activated and deactivated</p>","parent_name":"SpeechPipelineProfiles"},"Enums/SpeechPipelineProfiles.html#/c:@M@Spokestack@E@SpeechPipelineProfiles@SpeechPipelineProfilesTfLiteWakewordKeyword":{"name":"tfLiteWakewordKeyword","abstract":"<p>VAD-sensitive TFLiteWakeword activates TFLite Keyword Recognizer</p>","parent_name":"SpeechPipelineProfiles"},"Enums/SpeechPipelineProfiles.html#/c:@M@Spokestack@E@SpeechPipelineProfiles@SpeechPipelineProfilesVadTriggerKeyword":{"name":"vadTriggerKeyword","abstract":"<p>VAD-triggered TFLite Keyword Recognizer</p>","parent_name":"SpeechPipelineProfiles"},"Enums/SpeechPipelineProfiles.html#/c:@M@Spokestack@E@SpeechPipelineProfiles@SpeechPipelineProfilesAppleWakewordAppleSpeech":{"name":"appleWakewordAppleSpeech","abstract":"<p>VAD-sensitive Apple wakeword activates Apple ASR</p>","parent_name":"SpeechPipelineProfiles"},"Enums/SpeechPipelineProfiles.html#/c:@M@Spokestack@E@SpeechPipelineProfiles@SpeechPipelineProfilesAppleWakewordKeyword":{"name":"appleWakewordKeyword","abstract":"<p>VAD-sensitive Apple wakeword activates TFLite Keyword Recognizer</p>","parent_name":"SpeechPipelineProfiles"},"Enums/NLUError.html#/s:10Spokestack8NLUErrorO20invalidConfigurationyACSScACmF":{"name":"invalidConfiguration(_:)","abstract":"<p>The NLUService instance was configured with incompatible settings.</p>","parent_name":"NLUError"},"Enums/NLUError.html#/s:10Spokestack8NLUErrorO9tokenizeryACSScACmF":{"name":"tokenizer(_:)","abstract":"<p>The NLUService tokenizer encountered an error.</p>","parent_name":"NLUError"},"Enums/NLUError.html#/s:10Spokestack8NLUErrorO5modelyACSScACmF":{"name":"model(_:)","abstract":"<p>The model provided to the NLUService instance encountered an error.</p>","parent_name":"NLUError"},"Enums/NLUError.html#/s:10Spokestack8NLUErrorO8metadatayACSScACmF":{"name":"metadata(_:)","abstract":"<p>There was a problem with the metadata provided to the NLUService instance.</p>","parent_name":"NLUError"},"Enums/NLUError.html#/s:10Spokestack8NLUErrorO16errorDescriptionSSSgvp":{"name":"errorDescription","abstract":"<p><code>LocalizedError</code> implementation so that <code>localizedDescription</code> isn&rsquo;t an enum index.</p>","parent_name":"NLUError"},"Enums/CommandModelError.html#/s:10Spokestack17CommandModelErrorO5modelyACSScACmF":{"name":"model(_:)","abstract":"<p>The command recognizer was unable to configure the recognizer model(s).</p>","parent_name":"CommandModelError"},"Enums/CommandModelError.html#/s:10Spokestack17CommandModelErrorO7processyACSScACmF":{"name":"process(_:)","abstract":"<p>The command recognizer encountered an error during the processing of the audio frame.</p>","parent_name":"CommandModelError"},"Enums/CommandModelError.html#/s:10Spokestack17CommandModelErrorO6filteryACSScACmF":{"name":"filter(_:)","abstract":"<p>The command recognizer encountered an error during the configuration or running of the filter model.</p>","parent_name":"CommandModelError"},"Enums/CommandModelError.html#/s:10Spokestack17CommandModelErrorO6encodeyACSScACmF":{"name":"encode(_:)","abstract":"<p>The command recognizer encountered an error during the configuration or running of the encode model.</p>","parent_name":"CommandModelError"},"Enums/CommandModelError.html#/s:10Spokestack17CommandModelErrorO6detectyACSScACmF":{"name":"detect(_:)","abstract":"<p>The command recognizer encountered an error during the configuration or running of the detect model.</p>","parent_name":"CommandModelError"},"Enums/CommandModelError.html#/s:10Spokestack17CommandModelErrorO16errorDescriptionSSSgvp":{"name":"errorDescription","abstract":"<p><code>LocalizedError</code> implementation so that <code>localizedDescription</code> isn&rsquo;t an enum index.</p>","parent_name":"CommandModelError"},"Enums/VADError.html#/s:10Spokestack8VADErrorO20invalidConfigurationyACSScACmF":{"name":"invalidConfiguration(_:)","abstract":"<p>The VAD instance was configured with incompatible settings.</p>","parent_name":"VADError"},"Enums/VADError.html#/s:10Spokestack8VADErrorO14initializationyACSScACmF":{"name":"initialization(_:)","abstract":"<p>The VAD instance was unable to initialize.</p>","parent_name":"VADError"},"Enums/VADError.html#/s:10Spokestack8VADErrorO10processingyACSScACmF":{"name":"processing(_:)","abstract":"<p>The VAD instance encountered an error during the processing of the audio frame.</p>","parent_name":"VADError"},"Enums/VADError.html#/s:10Spokestack8VADErrorO16errorDescriptionSSSgvp":{"name":"errorDescription","abstract":"<p><code>LocalizedError</code> implementation so that <code>localizedDescription</code> isn&rsquo;t an enum index.</p>","parent_name":"VADError"},"Enums/SpeechPipelineError.html#/s:10Spokestack19SpeechPipelineErrorO12illegalStateyACSScACmF":{"name":"illegalState(_:)","abstract":"<p>The SpeechPipeline internal buffers entered an illegal state.</p>","parent_name":"SpeechPipelineError"},"Enums/SpeechPipelineError.html#/s:10Spokestack19SpeechPipelineErrorO15invalidResponseyACSScACmF":{"name":"invalidResponse(_:)","abstract":"<p>The SpeechPipeline received a response that was invalid.</p>","parent_name":"SpeechPipelineError"},"Enums/SpeechPipelineError.html#/s:10Spokestack19SpeechPipelineErrorO7failureyACSScACmF":{"name":"failure(_:)","abstract":"<p>The SpeechPipeline encountered a failure in a component.</p>","parent_name":"SpeechPipelineError"},"Enums/SpeechPipelineError.html#/s:10Spokestack19SpeechPipelineErrorO11errorNotSetyACSScACmF":{"name":"errorNotSet(_:)","abstract":"<p>A pipeline component attempted to send an error to SpeechContext&rsquo;s listeners without first setting the SpeechContext.error property.</p>","parent_name":"SpeechPipelineError"},"Enums/SpeechPipelineError.html#/s:10Spokestack19SpeechPipelineErrorO17incompleteBuilderyACSScACmF":{"name":"incompleteBuilder(_:)","abstract":"<p>The settings provided to the pipeline builder were not sufficient to create a pipeline.</p>","parent_name":"SpeechPipelineError"},"Enums/SpeechPipelineError.html#/s:10Spokestack19SpeechPipelineErrorO6apiKeyyACSScACmF":{"name":"apiKey(_:)","abstract":"<p>The api key provided is not valid.</p>","parent_name":"SpeechPipelineError"},"Enums/SpeechPipelineError.html#/s:10Spokestack19SpeechPipelineErrorO16errorDescriptionSSSgvp":{"name":"errorDescription","abstract":"<p><code>LocalizedError</code> implementation so that <code>localizedDescription</code> isn&rsquo;t an enum index.</p>","parent_name":"SpeechPipelineError"},"Enums/AudioError.html#/s:10Spokestack10AudioErrorO17audioSessionSetupyACSScACmF":{"name":"audioSessionSetup(_:)","abstract":"<p>An audio unit system error</p>","parent_name":"AudioError"},"Enums/AudioError.html#/s:10Spokestack10AudioErrorO15audioControlleryACSScACmF":{"name":"audioController(_:)","abstract":"<p>An audio controller error</p>","parent_name":"AudioError"},"Enums/AudioError.html#/s:10Spokestack10AudioErrorO16errorDescriptionSSSgvp":{"name":"errorDescription","abstract":"<p><code>LocalizedError</code> implementation so that <code>localizedDescription</code> isn&rsquo;t an enum index.</p>","parent_name":"AudioError"},"Enums/AudioError.html":{"name":"AudioError","abstract":"<p>Errors thrown by <code>AudioController</code> during <code>startStreaming</code> and <code>stopStreaming</code>.</p>"},"Enums/SpeechPipelineError.html":{"name":"SpeechPipelineError","abstract":"<p>Errors thrown by the <code><a href=\"36f8f5912051ae747ef441d6511ca4cbClasses/SpeechPipeline.html\">SpeechPipeline</a></code>.</p>"},"Enums/VADError.html":{"name":"VADError","abstract":"<p>Errors thrown by the Voice Activity Detector.</p>"},"Enums/CommandModelError.html":{"name":"CommandModelError","abstract":"<p>Errors thrown by  command models.</p>"},"Enums/NLUError.html":{"name":"NLUError","abstract":"<p>Errors thrown by an NLUService instance.</p>"},"Enums/SpeechPipelineProfiles.html":{"name":"SpeechPipelineProfiles","abstract":"<p>Profiles that may be passed to <code><a href=\"36f8f5912051ae747ef441d6511ca4cbClasses/SpeechPipelineBuilder.html\">SpeechPipelineBuilder</a></code> for easy pipeline configuring.</p>"},"Enums/TTSInputFormat.html":{"name":"TTSInputFormat","abstract":"<p>Designate the format the input is provided in.</p>"},"Enums/VADMode.html":{"name":"VADMode","abstract":"<p>Indicate how likely it is that non-speech will activate the VAD.</p>"},"Classes/WebRTCVAD.html#/c:@M@Spokestack@objc(cs)WebRTCVAD(py)configuration":{"name":"configuration","abstract":"<p>Configuration for the detector.</p>","parent_name":"WebRTCVAD"},"Classes/WebRTCVAD.html#/c:@M@Spokestack@objc(cs)WebRTCVAD(py)context":{"name":"context","abstract":"<p>Global state for the speech pipeline.</p>","parent_name":"WebRTCVAD"},"Classes/WebRTCVAD.html#/c:@M@Spokestack@objc(cs)WebRTCVAD(im)startStreaming":{"name":"startStreaming()","abstract":"<p>Triggered by the speech pipeline, instructing the detector to begin streaming and processing audio.</p>","parent_name":"WebRTCVAD"},"Classes/WebRTCVAD.html#/c:@M@Spokestack@objc(cs)WebRTCVAD(im)stopStreaming":{"name":"stopStreaming()","abstract":"<p>Triggered by the speech pipeline, instructing the detector to stop streaming audio and complete processing.</p>","parent_name":"WebRTCVAD"},"Classes/WebRTCVAD.html#/c:@M@Spokestack@objc(cs)WebRTCVAD(im)init:context:":{"name":"init(_:context:)","abstract":"<p>Initializes a WebRTCVAD instance.</p>","parent_name":"WebRTCVAD"},"Classes/WebRTCVAD.html#/c:@M@Spokestack@objc(cs)WebRTCVAD(im)process:":{"name":"process(_:)","abstract":"<p>Processes an audio frame, detecting speech.</p>","parent_name":"WebRTCVAD"},"Classes/VADTrigger.html#/c:@M@Spokestack@objc(cs)VADTrigger(py)configuration":{"name":"configuration","abstract":"<p>Configuration for the trigger.</p>","parent_name":"VADTrigger"},"Classes/VADTrigger.html#/c:@M@Spokestack@objc(cs)VADTrigger(py)context":{"name":"context","abstract":"<p>Global state for the speech pipeline.</p>","parent_name":"VADTrigger"},"Classes/VADTrigger.html#/c:@M@Spokestack@objc(cs)VADTrigger(im)init:context:":{"name":"init(_:context:)","abstract":"<p>Initializes a VADTrigger instance. A wakeword trigger is initialized by, and receives <code>startStreaming</code> and <code>stopStreaming</code> events from, an instance of <code><a href=\"36f8f5912051ae747ef441d6511ca4cbClasses/SpeechPipeline.html\">SpeechPipeline</a></code>. The VADTrigger receives audio data frames to <code>process</code> from <code>AudioController</code>.</p>","parent_name":"VADTrigger"},"Classes/VADTrigger.html#/c:@M@Spokestack@objc(cs)VADTrigger(im)startStreaming":{"name":"startStreaming()","abstract":"<p>Triggered by the speech pipeline, instructing the trigger to begin streaming and processing audio.</p>","parent_name":"VADTrigger"},"Classes/VADTrigger.html#/c:@M@Spokestack@objc(cs)VADTrigger(im)stopStreaming":{"name":"stopStreaming()","abstract":"<p>Triggered by the speech pipeline, instructing the trigger to stop streaming audio and complete processing.</p>","parent_name":"VADTrigger"},"Classes/VADTrigger.html#/c:@M@Spokestack@objc(cs)VADTrigger(im)process:":{"name":"process(_:)","abstract":"<p>Processes an audio frame, activating the pipeline if speech is detected.</p>","parent_name":"VADTrigger"},"Classes/TextToSpeechResult.html#/c:@M@Spokestack@objc(cs)TextToSpeechResult(py)url":{"name":"url","abstract":"<p>Undocumented</p>","parent_name":"TextToSpeechResult"},"Classes/TextToSpeechResult.html#/c:@M@Spokestack@objc(cs)TextToSpeechResult(py)id":{"name":"id","abstract":"<p>Undocumented</p>","parent_name":"TextToSpeechResult"},"Classes/TextToSpeechResult.html#/c:@M@Spokestack@objc(cs)TextToSpeechResult(im)initWithId:url:":{"name":"init(id:url:)","abstract":"<p>Undocumented</p>","parent_name":"TextToSpeechResult"},"Classes/TextToSpeechInput.html#/c:@M@Spokestack@objc(cs)TextToSpeechInput(im)init:voice:inputFormat:id:":{"name":"init(_:voice:inputFormat:id:)","abstract":"<p>Initializer for a new TextToSpeechInput instance.</p>","parent_name":"TextToSpeechInput"},"Classes/TextToSpeechInput.html#/c:@M@Spokestack@objc(cs)TextToSpeechInput(py)voice":{"name":"voice","abstract":"<p>The synthetic voice used to generate speech.</p>","parent_name":"TextToSpeechInput"},"Classes/TextToSpeechInput.html#/c:@M@Spokestack@objc(cs)TextToSpeechInput(py)input":{"name":"input","abstract":"<p>The input to the synthetic voice.</p>","parent_name":"TextToSpeechInput"},"Classes/TextToSpeechInput.html#/c:@M@Spokestack@objc(cs)TextToSpeechInput(py)inputFormat":{"name":"inputFormat","abstract":"<p>The formatting of the input.</p>","parent_name":"TextToSpeechInput"},"Classes/TextToSpeechInput.html#/c:@M@Spokestack@objc(cs)TextToSpeechInput(py)id":{"name":"id","abstract":"<p>A unique identifier for this input request.</p>","parent_name":"TextToSpeechInput"},"Classes/TextToSpeech.html#/s:10Spokestack12TextToSpeechC9delegatesSayAA0A8Delegate_pGvp":{"name":"delegates","abstract":"<p>Delegate that receives TTS events.</p>","parent_name":"TextToSpeech"},"Classes/TextToSpeech.html#/c:@M@Spokestack@objc(cs)TextToSpeech(im)initWithConfiguration:error:":{"name":"init(configuration:)","abstract":"<p>Initializes a new text to speech instance without a delegate.</p>","parent_name":"TextToSpeech"},"Classes/TextToSpeech.html#/c:@M@Spokestack@objc(cs)TextToSpeech(im)init:configuration:":{"name":"init(_:configuration:)","abstract":"<p>Initializes a new text to speech instance.</p>","parent_name":"TextToSpeech"},"Classes/TextToSpeech.html#/c:@M@Spokestack@objc(cs)TextToSpeech(im)speak:":{"name":"speak(_:)","abstract":"<p>Synthesize speech using the provided input parameters and speech configuration, and play back the result using the default audio system.</p>","parent_name":"TextToSpeech"},"Classes/TextToSpeech.html#/c:@M@Spokestack@objc(cs)TextToSpeech(im)stopSpeaking":{"name":"stopSpeaking()","abstract":"<p>Stops playback of the current synthesis result.</p>","parent_name":"TextToSpeech"},"Classes/TextToSpeech.html#/c:@M@Spokestack@objc(cs)TextToSpeech(im)synthesize:":{"name":"synthesize(_:)","abstract":"<p>Synthesize speech using the provided input parameters and speech configuration. A successful synthesis will return a URL to the streaming audio container of synthesized speech to the <code>TextToSpeech</code>&lsquo;s <code>delegate</code>.</p>","parent_name":"TextToSpeech"},"Classes/TextToSpeech.html#/s:10Spokestack12TextToSpeechC10synthesizey7Combine12AnyPublisherVySayAA0bcD6ResultCGs5Error_pGSayAA0bcD5InputCGF":{"name":"synthesize(_:)","abstract":"<p>Synthesize speech using the provided list of inputs. A successful set of synthesises returns a list of synthesis results.</p>","parent_name":"TextToSpeech"},"Classes/TextToSpeech.html#/c:@M@Spokestack@objc(cs)TextToSpeech(im)observeValueForKeyPath:ofObject:change:context:":{"name":"observeValue(forKeyPath:of:change:context:)","abstract":"<p>Internal function that must be public for Objective-C compatibility reasons.</p>","parent_name":"TextToSpeech"},"Classes/TFLiteWakewordRecognizer.html#/c:@M@Spokestack@objc(cs)TFLiteWakewordRecognizer(py)configuration":{"name":"configuration","abstract":"<p>Configuration for the recognizer.</p>","parent_name":"TFLiteWakewordRecognizer"},"Classes/TFLiteWakewordRecognizer.html#/c:@M@Spokestack@objc(cs)TFLiteWakewordRecognizer(py)context":{"name":"context","abstract":"<p>Global state for the speech pipeline.</p>","parent_name":"TFLiteWakewordRecognizer"},"Classes/TFLiteWakewordRecognizer.html#/c:@M@Spokestack@objc(cs)TFLiteWakewordRecognizer(im)init:context:":{"name":"init(_:context:)","abstract":"<p>Initializes a TFLiteWakewordRecognizer instance.</p>","parent_name":"TFLiteWakewordRecognizer"},"Classes/TFLiteWakewordRecognizer.html#/c:@CM@Spokestack@objc(cs)TFLiteWakewordRecognizer(im)startStreaming":{"name":"startStreaming()","abstract":"<p>Triggered by the speech pipeline, instructing the recognizer to begin streaming and processing audio.</p>","parent_name":"TFLiteWakewordRecognizer"},"Classes/TFLiteWakewordRecognizer.html#/c:@CM@Spokestack@objc(cs)TFLiteWakewordRecognizer(im)stopStreaming":{"name":"stopStreaming()","abstract":"<p>Triggered by the speech pipeline, instructing the recognizer to stop streaming audio and complete processing.</p>","parent_name":"TFLiteWakewordRecognizer"},"Classes/TFLiteWakewordRecognizer.html#/c:@CM@Spokestack@objc(cs)TFLiteWakewordRecognizer(im)process:":{"name":"process(_:)","abstract":"<p>Receives a frame of audio samples for processing. Interface between the <code><a href=\"36f8f5912051ae747ef441d6511ca4cbProtocols/SpeechProcessor.html\">SpeechProcessor</a></code> and <code>AudioController</code> components.</p>","parent_name":"TFLiteWakewordRecognizer"},"Classes/TFLiteKeywordRecognizer.html#/c:@M@Spokestack@objc(cs)TFLiteKeywordRecognizer(py)configuration":{"name":"configuration","abstract":"<p>Configuration for the recognizer.</p>","parent_name":"TFLiteKeywordRecognizer"},"Classes/TFLiteKeywordRecognizer.html#/c:@M@Spokestack@objc(cs)TFLiteKeywordRecognizer(py)context":{"name":"context","abstract":"<p>Global state for the speech pipeline.</p>","parent_name":"TFLiteKeywordRecognizer"},"Classes/TFLiteKeywordRecognizer.html#/c:@M@Spokestack@objc(cs)TFLiteKeywordRecognizer(im)init:context:":{"name":"init(_:context:)","abstract":"<p>Initializes a TFLiteKeywordRecognizer instance.</p>","parent_name":"TFLiteKeywordRecognizer"},"Classes/TFLiteKeywordRecognizer.html#/c:@M@Spokestack@objc(pl)SpeechProcessor(im)startStreaming":{"name":"startStreaming()","parent_name":"TFLiteKeywordRecognizer"},"Classes/TFLiteKeywordRecognizer.html#/c:@M@Spokestack@objc(pl)SpeechProcessor(im)stopStreaming":{"name":"stopStreaming()","parent_name":"TFLiteKeywordRecognizer"},"Classes/TFLiteKeywordRecognizer.html#/c:@M@Spokestack@objc(pl)SpeechProcessor(im)process:":{"name":"process(_:)","parent_name":"TFLiteKeywordRecognizer"},"Classes/SpokestackSpeechRecognizer.html#/c:@M@Spokestack@objc(cs)SpokestackSpeechRecognizer(py)configuration":{"name":"configuration","abstract":"<p>Configuration for the recognizer.</p>","parent_name":"SpokestackSpeechRecognizer"},"Classes/SpokestackSpeechRecognizer.html#/c:@M@Spokestack@objc(cs)SpokestackSpeechRecognizer(py)context":{"name":"context","abstract":"<p>Global state for the speech pipeline.</p>","parent_name":"SpokestackSpeechRecognizer"},"Classes/SpokestackSpeechRecognizer.html#/c:@M@Spokestack@objc(cs)SpokestackSpeechRecognizer(im)init:context:":{"name":"init(_:context:)","abstract":"<p>Initializes an instance of SpokestackSpeechRecognizer.</p>","parent_name":"SpokestackSpeechRecognizer"},"Classes/SpokestackSpeechRecognizer.html#/c:@CM@Spokestack@objc(cs)SpokestackSpeechRecognizer(im)startStreaming":{"name":"startStreaming()","abstract":"<p>Triggered by the speech pipeline, instructing the recognizer to begin streaming and processing audio.</p>","parent_name":"SpokestackSpeechRecognizer"},"Classes/SpokestackSpeechRecognizer.html#/c:@CM@Spokestack@objc(cs)SpokestackSpeechRecognizer(im)stopStreaming":{"name":"stopStreaming()","abstract":"<p>Triggered by the speech pipeline, instructing the recognizer to stop streaming audio and complete processing.</p>","parent_name":"SpokestackSpeechRecognizer"},"Classes/SpokestackSpeechRecognizer.html#/c:@CM@Spokestack@objc(cs)SpokestackSpeechRecognizer(im)process:":{"name":"process(_:)","abstract":"<p>Receives a frame of audio samples for processing. Interface between the <code><a href=\"36f8f5912051ae747ef441d6511ca4cbProtocols/SpeechProcessor.html\">SpeechProcessor</a></code> and <code>AudioController</code> components.</p>","parent_name":"SpokestackSpeechRecognizer"},"Classes/SpokestackBuilder.html#/c:@M@Spokestack@objc(cs)SpokestackBuilder(im)init":{"name":"init()","abstract":"<p>Create a Spokestack builder with a default configuration.</p>","parent_name":"SpokestackBuilder"},"Classes/SpokestackBuilder.html#/c:@M@Spokestack@objc(cs)SpokestackBuilder(im)addDelegate:":{"name":"addDelegate(_:)","abstract":"<p>Delegate events will be sent to the specified listener.</p>","parent_name":"SpokestackBuilder"},"Classes/SpokestackBuilder.html#/c:@M@Spokestack@objc(cs)SpokestackBuilder(im)usePipelineProfile:":{"name":"usePipelineProfile(_:)","abstract":"<p>Applies configuration from <code><a href=\"36f8f5912051ae747ef441d6511ca4cbEnums/SpeechPipelineProfiles.html\">SpeechPipelineProfiles</a></code> to the current builder, returning the modified builder.</p>","parent_name":"SpokestackBuilder"},"Classes/SpokestackBuilder.html#/c:@M@Spokestack@objc(cs)SpokestackBuilder(im)setProperty::":{"name":"setProperty(_:_:)","abstract":"<p>Sets a <code><a href=\"36f8f5912051ae747ef441d6511ca4cbClasses/SpeechConfiguration.html\">SpeechConfiguration</a></code> configuration value.</p>","parent_name":"SpokestackBuilder"},"Classes/SpokestackBuilder.html#/c:@M@Spokestack@objc(cs)SpokestackBuilder(im)setConfiguration:":{"name":"setConfiguration(_:)","abstract":"<p>Replaces the default speech configuration with the specified configuration.</p>","parent_name":"SpokestackBuilder"},"Classes/SpokestackBuilder.html#/c:@M@Spokestack@objc(cs)SpokestackBuilder(im)setDelegateDispatchQueue:":{"name":"setDelegateDispatchQueue(_:)","abstract":"<p>Delegate events will be sent using the specified dispatch queue.</p>","parent_name":"SpokestackBuilder"},"Classes/SpokestackBuilder.html#/c:@M@Spokestack@objc(cs)SpokestackBuilder(im)setTranscriptEditor:":{"name":"setTranscriptEditor(_:)","abstract":"<p>Sets a transcript editor used to alter ASR transcripts before they are classified by the NLU subsystem.</p>","parent_name":"SpokestackBuilder"},"Classes/SpokestackBuilder.html#/c:@M@Spokestack@objc(cs)SpokestackBuilder(im)buildAndReturnError:":{"name":"build()","abstract":"<p>Build this configuration into a <code><a href=\"36f8f5912051ae747ef441d6511ca4cbClasses/Spokestack.html\">Spokestack</a></code> instance.</p>","parent_name":"SpokestackBuilder"},"Classes/Spokestack.html#/c:@M@Spokestack@objc(cs)Spokestack(py)pipeline":{"name":"pipeline","abstract":"<p>This is the client entry point to the Spokestack voice input system.</p>","parent_name":"Spokestack"},"Classes/Spokestack.html#/c:@M@Spokestack@objc(cs)Spokestack(py)tts":{"name":"tts","abstract":"<p>This is the client entry point for the Spokestack Text to Speech (TTS) system.</p>","parent_name":"Spokestack"},"Classes/Spokestack.html#/c:@M@Spokestack@objc(cs)Spokestack(py)nlu":{"name":"nlu","abstract":"<p>This is the client entry point for the Spokestack BERT NLU implementation.</p>","parent_name":"Spokestack"},"Classes/Spokestack.html#/c:@M@Spokestack@objc(cs)Spokestack(py)context":{"name":"context","abstract":"<p>Maintains global state for the speech pipeline.</p>","parent_name":"Spokestack"},"Classes/Spokestack.html#/c:@M@Spokestack@objc(cs)Spokestack(py)configuration":{"name":"configuration","abstract":"<p>Configuration properties for Spokestack modules.</p>","parent_name":"Spokestack"},"Classes/Spokestack.html#/c:@CM@Spokestack@objc(cs)Spokestack(im)failureWithError:":{"name":"failure(error:)","abstract":"<p>A required function for <code><a href=\"36f8f5912051ae747ef441d6511ca4cbProtocols/SpokestackDelegate.html\">SpokestackDelegate</a></code> implementors</p>","parent_name":"Spokestack"},"Classes/Spokestack.html#/c:@CM@Spokestack@objc(cs)Spokestack(im)didRecognize:":{"name":"didRecognize(_:)","abstract":"<p>An event receiver used to fulfill automatic classification configuration.</p>","parent_name":"Spokestack"},"Classes/SpeechPipelineBuilder.html#/c:@M@Spokestack@objc(cs)SpeechPipelineBuilder(im)useProfile:":{"name":"useProfile(_:)","abstract":"<p>Applies configuration from <code><a href=\"36f8f5912051ae747ef441d6511ca4cbEnums/SpeechPipelineProfiles.html\">SpeechPipelineProfiles</a></code> to the current builder, returning the modified builder.</p>","parent_name":"SpeechPipelineBuilder"},"Classes/SpeechPipelineBuilder.html#/c:@M@Spokestack@objc(cs)SpeechPipelineBuilder(im)setProperty::":{"name":"setProperty(_:_:)","abstract":"<p>Sets a <code><a href=\"36f8f5912051ae747ef441d6511ca4cbClasses/SpeechConfiguration.html\">SpeechConfiguration</a></code> configuration value.</p>","parent_name":"SpeechPipelineBuilder"},"Classes/SpeechPipelineBuilder.html#/c:@M@Spokestack@objc(cs)SpeechPipelineBuilder(im)setConfiguration:":{"name":"setConfiguration(_:)","abstract":"<p>Replaces the default speech configuration with the specified configuration.</p>","parent_name":"SpeechPipelineBuilder"},"Classes/SpeechPipelineBuilder.html#/c:@M@Spokestack@objc(cs)SpeechPipelineBuilder(im)setDelegateDispatchQueue:":{"name":"setDelegateDispatchQueue(_:)","abstract":"<p>Delegate events will be sent using the specified dispatch queue.</p>","parent_name":"SpeechPipelineBuilder"},"Classes/SpeechPipelineBuilder.html#/c:@M@Spokestack@objc(cs)SpeechPipelineBuilder(im)addListener:":{"name":"addListener(_:)","abstract":"<p>Delegate events will be sent to the specified listener.</p>","parent_name":"SpeechPipelineBuilder"},"Classes/SpeechPipelineBuilder.html#/c:@M@Spokestack@objc(cs)SpeechPipelineBuilder(im)buildAndReturnError:":{"name":"build()","abstract":"<p>Build this configuration into a <code><a href=\"36f8f5912051ae747ef441d6511ca4cbClasses/SpeechPipeline.html\">SpeechPipeline</a></code> instance.</p>","parent_name":"SpeechPipelineBuilder"},"Classes/SpeechPipeline.html#/c:@M@Spokestack@objc(cs)SpeechPipeline(py)configuration":{"name":"configuration","abstract":"<p>Pipeline configuration parameters.</p>","parent_name":"SpeechPipeline"},"Classes/SpeechPipeline.html#/c:@M@Spokestack@objc(cs)SpeechPipeline(py)context":{"name":"context","abstract":"<p>Global state for the speech pipeline.</p>","parent_name":"SpeechPipeline"},"Classes/SpeechPipeline.html#/c:@M@Spokestack@objc(cs)SpeechPipeline(im)initWithConfiguration:listeners:stages:context:":{"name":"init(configuration:listeners:stages:context:)","abstract":"<p>Initializes a new speech pipeline instance. For use by clients wishing to pass 3rd-party stages to the spokestack pipeline.</p>","parent_name":"SpeechPipeline"},"Classes/SpeechPipeline.html#/c:@M@Spokestack@objc(cs)SpeechPipeline(im)activate":{"name":"activate()","abstract":"<p>Activates speech recognition. The pipeline remains active until the user stops talking or the activation timeout is reached.</p>","parent_name":"SpeechPipeline"},"Classes/SpeechPipeline.html#/c:@M@Spokestack@objc(cs)SpeechPipeline(im)deactivate":{"name":"deactivate()","abstract":"<p>Deactivates speech recognition.  The pipeline returns to awaiting either wakeword activation or an explicit <code>activate</code> call.</p>","parent_name":"SpeechPipeline"},"Classes/SpeechPipeline.html#/c:@M@Spokestack@objc(cs)SpeechPipeline(im)start":{"name":"start()","abstract":"<p>Starts  the speech pipeline.</p>","parent_name":"SpeechPipeline"},"Classes/SpeechPipeline.html#/c:@M@Spokestack@objc(cs)SpeechPipeline(im)stop":{"name":"stop()","abstract":"<p>Stops the speech pipeline.</p>","parent_name":"SpeechPipeline"},"Classes/SpeechContext.html#/c:@M@Spokestack@objc(cs)SpeechContext(py)configuration":{"name":"configuration","abstract":"<p>Undocumented</p>","parent_name":"SpeechContext"},"Classes/SpeechContext.html#/c:@M@Spokestack@objc(cs)SpeechContext(py)transcript":{"name":"transcript","abstract":"<p>Current speech transcript</p>","parent_name":"SpeechContext"},"Classes/SpeechContext.html#/c:@M@Spokestack@objc(cs)SpeechContext(py)confidence":{"name":"confidence","abstract":"<p>Current speech recognition confidence: [0-1)</p>","parent_name":"SpeechContext"},"Classes/SpeechContext.html#/c:@M@Spokestack@objc(cs)SpeechContext(py)isActive":{"name":"isActive","abstract":"<p>Speech recognition active indicator</p>","parent_name":"SpeechContext"},"Classes/SpeechContext.html#/c:@M@Spokestack@objc(cs)SpeechContext(py)isSpeech":{"name":"isSpeech","abstract":"<p>Speech detected indicator</p>","parent_name":"SpeechContext"},"Classes/SpeechContext.html#/c:@M@Spokestack@objc(cs)SpeechContext(im)init:":{"name":"init(_:)","abstract":"<p>Initializes a speech context instance using the specified speech pipeline configuration.</p>","parent_name":"SpeechContext"},"Classes/SpeechConfiguration.html#/c:@M@Spokestack@objc(cs)SpeechConfiguration(py)wakewords":{"name":"wakewords","abstract":"<p>A comma-separated list of wakeword keywords</p>","parent_name":"SpeechConfiguration"},"Classes/SpeechConfiguration.html#/s:10Spokestack19SpeechConfigurationC13fftWindowTypeAA16SignalProcessingV09FFTWindowF0Ovp":{"name":"fftWindowType","abstract":"<p>The name of the window function to apply to each audio frame before calculating the STFT.</p>","parent_name":"SpeechConfiguration"},"Classes/SpeechConfiguration.html#/c:@M@Spokestack@objc(cs)SpeechConfiguration(py)rmsTarget":{"name":"rmsTarget","abstract":"<p>The desired linear Root Mean Squared (RMS) signal energy, which is used for signal normalization and should be tuned to the RMS target used during wakeword model training.</p>","parent_name":"SpeechConfiguration"},"Classes/SpeechConfiguration.html#/c:@M@Spokestack@objc(cs)SpeechConfiguration(py)rmsAlpha":{"name":"rmsAlpha","abstract":"<p>The Exponentially Weighted Moving Average (EWMA) update rate for the current  Root Mean Squared (RMS) signal energy (0 for no RMS normalization).</p>","parent_name":"SpeechConfiguration"},"Classes/SpeechConfiguration.html#/c:@M@Spokestack@objc(cs)SpeechConfiguration(py)fftWindowSize":{"name":"fftWindowSize","abstract":"<p>The size of the signal window used to calculate the STFT, in number of samples - should be a power of 2 for maximum efficiency.</p>","parent_name":"SpeechConfiguration"},"Classes/SpeechConfiguration.html#/c:@M@Spokestack@objc(cs)SpeechConfiguration(py)fftHopLength":{"name":"fftHopLength","abstract":"<p>The length of time to skip each time the overlapping STFT is calculated, in milliseconds.</p>","parent_name":"SpeechConfiguration"},"Classes/SpeechConfiguration.html#/c:@M@Spokestack@objc(cs)SpeechConfiguration(py)melFrameLength":{"name":"melFrameLength","abstract":"<p>The length of a frame in the mel spectrogram used as an input to the wakeword recognizer encoder, in milliseconds.</p>","parent_name":"SpeechConfiguration"},"Classes/SpeechConfiguration.html#/c:@M@Spokestack@objc(cs)SpeechConfiguration(py)melFrameWidth":{"name":"melFrameWidth","abstract":"<p>The number of filterbank components in each mel spectrogram frame sent to the wakeword recognizer.</p>","parent_name":"SpeechConfiguration"},"Classes/SpeechConfiguration.html#/c:@M@Spokestack@objc(cs)SpeechConfiguration(py)stateWidth":{"name":"stateWidth","abstract":"<p>The size of the wakeword recognizer&rsquo;s encoder state output.</p>","parent_name":"SpeechConfiguration"},"Classes/SpeechConfiguration.html#/c:@M@Spokestack@objc(cs)SpeechConfiguration(py)encodeWidth":{"name":"encodeWidth","abstract":"<p>The size of the wakeword recognizer&rsquo;s encoder window output.</p>","parent_name":"SpeechConfiguration"},"Classes/SpeechConfiguration.html#/c:@M@Spokestack@objc(cs)SpeechConfiguration(py)encodeLength":{"name":"encodeLength","abstract":"<p>The length of the sliding window of encoder output used as an input to the wakeword recognizer classifier, in milliseconds.</p>","parent_name":"SpeechConfiguration"},"Classes/SpeechConfiguration.html#/c:@M@Spokestack@objc(cs)SpeechConfiguration(py)wakeThreshold":{"name":"wakeThreshold","abstract":"<p>The threshold of the wakeword recognizer classifier&rsquo;s posterior output, above which the wakeword recognizer activates the pipeline, in the range [0, 1].</p>","parent_name":"SpeechConfiguration"},"Classes/SpeechConfiguration.html#/c:@M@Spokestack@objc(cs)SpeechConfiguration(py)wakeActiveMin":{"name":"wakeActiveMin","abstract":"<p>The minimum length of an activation, in milliseconds. Used to ignore a Voice Activity Detector (VAD) deactivation after the wakeword.</p>","parent_name":"SpeechConfiguration"},"Classes/SpeechConfiguration.html#/c:@M@Spokestack@objc(cs)SpeechConfiguration(py)wakeActiveMax":{"name":"wakeActiveMax","abstract":"<p>The maximum length of an activation, in milliseconds. Used to time out the speech pipeline activation.</p>","parent_name":"SpeechConfiguration"},"Classes/SpeechConfiguration.html#/s:10Spokestack19SpeechConfigurationC7vadModeAA7VADModeOvp":{"name":"vadMode","abstract":"<p>Indicate to the VAD the level of permissiveness to non-speech activation.</p>","parent_name":"SpeechConfiguration"},"Classes/SpeechConfiguration.html#/c:@M@Spokestack@objc(cs)SpeechConfiguration(py)vadFallDelay":{"name":"vadFallDelay","abstract":"<p>Delay between a VAD deactivation and the delivery of the recognition results.</p>","parent_name":"SpeechConfiguration"},"Classes/SpeechConfiguration.html#/s:10Spokestack19SpeechConfigurationC10sampleRateSivp":{"name":"sampleRate","abstract":"<p>Audio sampling rate, in Hz.</p>","parent_name":"SpeechConfiguration"},"Classes/SpeechConfiguration.html#/c:@M@Spokestack@objc(cs)SpeechConfiguration(py)frameWidth":{"name":"frameWidth","abstract":"<p>Audio frame width, in milliseconds.</p>","parent_name":"SpeechConfiguration"},"Classes/SpeechConfiguration.html#/c:@M@Spokestack@objc(cs)SpeechConfiguration(py)wakewordRequestTimeout":{"name":"wakewordRequestTimeout","abstract":"<p>Length of time to allow an Apple ASR request to run, in milliseconds.</p>","parent_name":"SpeechConfiguration"},"Classes/SpeechConfiguration.html#/c:@M@Spokestack@objc(cs)SpeechConfiguration(py)preEmphasis":{"name":"preEmphasis","abstract":"<p>The pre-emphasis filter weight to apply to the normalized audio signal, in a range of [0, 1].</p>","parent_name":"SpeechConfiguration"},"Classes/SpeechConfiguration.html#/c:@M@Spokestack@objc(cs)SpeechConfiguration(py)filterModelName":{"name":"filterModelName","abstract":"<p>The filename of the machine learning model used for the filtering step.</p>","parent_name":"SpeechConfiguration"},"Classes/SpeechConfiguration.html#/c:@M@Spokestack@objc(cs)SpeechConfiguration(py)encodeModelName":{"name":"encodeModelName","abstract":"<p>The filename of the machine learning model used for the encoding step.</p>","parent_name":"SpeechConfiguration"},"Classes/SpeechConfiguration.html#/c:@M@Spokestack@objc(cs)SpeechConfiguration(py)detectModelName":{"name":"detectModelName","abstract":"<p>The filename of the machine learning model used for the detect step.</p>","parent_name":"SpeechConfiguration"},"Classes/SpeechConfiguration.html#/c:@M@Spokestack@objc(cs)SpeechConfiguration(py)filterModelPath":{"name":"filterModelPath","abstract":"<p>The filesystem path to the machine learning model for the filtering step.</p>","parent_name":"SpeechConfiguration"},"Classes/SpeechConfiguration.html#/c:@M@Spokestack@objc(cs)SpeechConfiguration(py)encodeModelPath":{"name":"encodeModelPath","abstract":"<p>The filesystem path to the machine learning model for the encoding step.</p>","parent_name":"SpeechConfiguration"},"Classes/SpeechConfiguration.html#/c:@M@Spokestack@objc(cs)SpeechConfiguration(py)detectModelPath":{"name":"detectModelPath","abstract":"<p>The filesystem path to the machine learning model for the detect step.</p>","parent_name":"SpeechConfiguration"},"Classes/SpeechConfiguration.html#/c:@M@Spokestack@objc(cs)SpeechConfiguration(py)apiId":{"name":"apiId","abstract":"<p>Text To Speech API client identifier key.</p>","parent_name":"SpeechConfiguration"},"Classes/SpeechConfiguration.html#/c:@M@Spokestack@objc(cs)SpeechConfiguration(py)apiSecret":{"name":"apiSecret","abstract":"<p>Text To Speech API client secret key.</p>","parent_name":"SpeechConfiguration"},"Classes/SpeechConfiguration.html#/c:@M@Spokestack@objc(cs)SpeechConfiguration(py)nluVocabularyPath":{"name":"nluVocabularyPath","abstract":"<p>The filesystem path to the vocabulary used for tokenizer encoding.</p>","parent_name":"SpeechConfiguration"},"Classes/SpeechConfiguration.html#/c:@M@Spokestack@objc(cs)SpeechConfiguration(py)nluTerminatorTokenIndex":{"name":"nluTerminatorTokenIndex","abstract":"<p>The index in the vocabulary of the terminator token. Determined  by the NLU vocabulary.</p>","parent_name":"SpeechConfiguration"},"Classes/SpeechConfiguration.html#/c:@M@Spokestack@objc(cs)SpeechConfiguration(py)nluPaddingTokenIndex":{"name":"nluPaddingTokenIndex","abstract":"<p>The index in the vocabulary of the terminator token. Determined  by the NLU vocabulary.</p>","parent_name":"SpeechConfiguration"},"Classes/SpeechConfiguration.html#/c:@M@Spokestack@objc(cs)SpeechConfiguration(py)nluModelPath":{"name":"nluModelPath","abstract":"<p>The filesystem path to the machine learning model for Natural Language Understanding processing.</p>","parent_name":"SpeechConfiguration"},"Classes/SpeechConfiguration.html#/c:@M@Spokestack@objc(cs)SpeechConfiguration(py)nluModelMetadataPath":{"name":"nluModelMetadataPath","abstract":"<p>The filesystem path to the model metadata for Natural Language Understanding processing.</p>","parent_name":"SpeechConfiguration"},"Classes/SpeechConfiguration.html#/c:@M@Spokestack@objc(cs)SpeechConfiguration(py)nluMaxTokenLength":{"name":"nluMaxTokenLength","abstract":"<p>The maximum utterance length the NLU can process. Determined  by the NLU model.</p>","parent_name":"SpeechConfiguration"},"Classes/SpeechConfiguration.html#/c:@M@Spokestack@objc(cs)SpeechConfiguration(py)tracing":{"name":"tracing","abstract":"<p>Debugging trace levels, for simple filtering.</p>","parent_name":"SpeechConfiguration"},"Classes/SpeechConfiguration.html#/c:@M@Spokestack@objc(cs)SpeechConfiguration(py)delegateDispatchQueue":{"name":"delegateDispatchQueue","abstract":"<p>Delegate events will be sent using the specified dispatch queue.</p>","parent_name":"SpeechConfiguration"},"Classes/SpeechConfiguration.html#/c:@M@Spokestack@objc(cs)SpeechConfiguration(py)automaticallyClassifyTranscript":{"name":"automaticallyClassifyTranscript","abstract":"<p>Automatically run Spokestack&rsquo;s NLU classification on ASR transcripts for clients that use the <code><a href=\"36f8f5912051ae747ef441d6511ca4cbClasses/Spokestack.html\">Spokestack</a></code> facade.</p>","parent_name":"SpeechConfiguration"},"Classes/SpeechConfiguration.html#/c:@M@Spokestack@objc(cs)SpeechConfiguration(py)keywordFilterModelName":{"name":"keywordFilterModelName","abstract":"<p>The filename of the machine learning model used for the filtering step of the keyword recognizer.</p>","parent_name":"SpeechConfiguration"},"Classes/SpeechConfiguration.html#/c:@M@Spokestack@objc(cs)SpeechConfiguration(py)keywordEncodeModelName":{"name":"keywordEncodeModelName","abstract":"<p>The filename of the machine learning model used for the encoding step of the keyword recognizer.</p>","parent_name":"SpeechConfiguration"},"Classes/SpeechConfiguration.html#/c:@M@Spokestack@objc(cs)SpeechConfiguration(py)keywordDetectModelName":{"name":"keywordDetectModelName","abstract":"<p>The filename of the machine learning model used for the detect step of the keyword recognizer.</p>","parent_name":"SpeechConfiguration"},"Classes/SpeechConfiguration.html#/c:@M@Spokestack@objc(cs)SpeechConfiguration(py)keywordMetadataName":{"name":"keywordMetadataName","abstract":"<p>The filename of the model metadata for keyword recognition</p>","parent_name":"SpeechConfiguration"},"Classes/SpeechConfiguration.html#/c:@M@Spokestack@objc(cs)SpeechConfiguration(py)keywordFilterModelPath":{"name":"keywordFilterModelPath","abstract":"<p>The filesystem path to the machine learning model for the filtering step of the keyword recognizer.</p>","parent_name":"SpeechConfiguration"},"Classes/SpeechConfiguration.html#/c:@M@Spokestack@objc(cs)SpeechConfiguration(py)keywordEncodeModelPath":{"name":"keywordEncodeModelPath","abstract":"<p>The filesystem path to the machine learning model for the encoding step of the keyword recognizer.</p>","parent_name":"SpeechConfiguration"},"Classes/SpeechConfiguration.html#/c:@M@Spokestack@objc(cs)SpeechConfiguration(py)keywordDetectModelPath":{"name":"keywordDetectModelPath","abstract":"<p>The filesystem path to the machine learning model for the detect step of the keyword recognizer.</p>","parent_name":"SpeechConfiguration"},"Classes/SpeechConfiguration.html#/c:@M@Spokestack@objc(cs)SpeechConfiguration(py)keywordThreshold":{"name":"keywordThreshold","abstract":"<p>The threshold of the keyword recognizer&rsquo;s posterior output, above which the keyword recognizer emits a recognition event for the most probable keyword.</p>","parent_name":"SpeechConfiguration"},"Classes/SpeechConfiguration.html#/c:@M@Spokestack@objc(cs)SpeechConfiguration(py)keywordMetadataPath":{"name":"keywordMetadataPath","abstract":"<p>The filesystem path to the model metadata for keyword recognition</p>","parent_name":"SpeechConfiguration"},"Classes/SpeechConfiguration.html#/c:@M@Spokestack@objc(cs)SpeechConfiguration(py)keywords":{"name":"keywords","abstract":"<p>A comma-separated list of keywords to recognize.</p>","parent_name":"SpeechConfiguration"},"Classes/SpeechConfiguration.html#/s:10Spokestack19SpeechConfigurationC20keywordFFTWindowTypeAA16SignalProcessingV0eF0Ovp":{"name":"keywordFFTWindowType","abstract":"<p>The name of the window function to apply to each audio frame before calculating the STFT.</p>","parent_name":"SpeechConfiguration"},"Classes/SpeechConfiguration.html#/c:@M@Spokestack@objc(cs)SpeechConfiguration(py)keywordFFTWindowSize":{"name":"keywordFFTWindowSize","abstract":"<p>The size of the signal window used to calculate the STFT, in number of samples - should be a power of 2 for maximum efficiency.</p>","parent_name":"SpeechConfiguration"},"Classes/SpeechConfiguration.html#/c:@M@Spokestack@objc(cs)SpeechConfiguration(py)keywordFFTHopLength":{"name":"keywordFFTHopLength","abstract":"<p>The length of time to skip each time the overlapping STFT is calculated, in milliseconds.</p>","parent_name":"SpeechConfiguration"},"Classes/SpeechConfiguration.html#/c:@M@Spokestack@objc(cs)SpeechConfiguration(py)keywordMelFrameLength":{"name":"keywordMelFrameLength","abstract":"<p>The length of a frame in the mel spectrogram used as an input to the wakeword recognizer encoder, in milliseconds.</p>","parent_name":"SpeechConfiguration"},"Classes/SpeechConfiguration.html#/c:@M@Spokestack@objc(cs)SpeechConfiguration(py)keywordMelFrameWidth":{"name":"keywordMelFrameWidth","abstract":"<p>The number of filterbank components in each mel spectrogram frame sent to the wakeword recognizer.</p>","parent_name":"SpeechConfiguration"},"Classes/SpeechConfiguration.html#/c:@M@Spokestack@objc(cs)SpeechConfiguration(py)keywordEncodeWidth":{"name":"keywordEncodeWidth","abstract":"<p>The size of the wakeword recognizer&rsquo;s encoder window output.</p>","parent_name":"SpeechConfiguration"},"Classes/SpeechConfiguration.html#/c:@M@Spokestack@objc(cs)SpeechConfiguration(py)keywordEncodeLength":{"name":"keywordEncodeLength","abstract":"<p>The length of the sliding window of encoder output used as an input to the wakeword recognizer classifier, in milliseconds.</p>","parent_name":"SpeechConfiguration"},"Classes/SpeechConfiguration.html#/c:@M@Spokestack@objc(cs)SpeechConfiguration(py)semaphoreTimeout":{"name":"semaphoreTimeout","abstract":"<p>Timeout in seconds used for semaphore waits in the speech pipeline</p>","parent_name":"SpeechConfiguration"},"Classes/NLUTensorflow.html#/c:@M@Spokestack@objc(cs)NLUTensorflow(py)configuration":{"name":"configuration","abstract":"<p>Configuration parameters for the NLU.</p>","parent_name":"NLUTensorflow"},"Classes/NLUTensorflow.html#/c:@M@Spokestack@objc(cs)NLUTensorflow(py)delegates":{"name":"delegates","abstract":"<p>An implementation of NLUDelegate to receive NLU events.</p>","parent_name":"NLUTensorflow"},"Classes/NLUTensorflow.html#/c:@M@Spokestack@objc(cs)NLUTensorflow(im)initWithConfiguration:error:":{"name":"init(configuration:)","abstract":"<p>Initializes an NLU instance.</p>","parent_name":"NLUTensorflow"},"Classes/NLUTensorflow.html#/c:@M@Spokestack@objc(cs)NLUTensorflow(im)init:configuration:error:":{"name":"init(_:configuration:)","abstract":"<p>Initializes an NLU instance.</p>","parent_name":"NLUTensorflow"},"Classes/NLUTensorflow.html#/c:@M@Spokestack@objc(cs)NLUTensorflow(im)classifyWithUtterance:context:":{"name":"classify(utterance:context:)","abstract":"<p>Classifies the provided input. The classification results are sent to the instance&rsquo;s configured NLUDelegate.</p>","parent_name":"NLUTensorflow"},"Classes/NLUTensorflow.html#/s:10Spokestack13NLUTensorflowC8classify10utterances7context7Combine10PublishersO8SequenceVy_Says6ResultOyAA9NLUResultCs5Error_pGGs5NeverOGSaySSG_SDySSypGtF":{"name":"classify(utterances:context:)","abstract":"<p>Classifies the provided input. NLUResult is sent to all subscribers.</p>","parent_name":"NLUTensorflow"},"Classes/Slot.html#/c:@M@Spokestack@objc(cs)Slot(py)type":{"name":"type","abstract":"<p>The underlying type of the slot value.</p>","parent_name":"Slot"},"Classes/Slot.html#/c:@M@Spokestack@objc(cs)Slot(py)value":{"name":"value","abstract":"<p>The slot&rsquo;s value.</p>","parent_name":"Slot"},"Classes/Slot.html#/c:@M@Spokestack@objc(cs)Slot(py)rawValue":{"name":"rawValue","abstract":"<p>Undocumented</p>","parent_name":"Slot"},"Classes/Slot.html#/c:@M@Spokestack@objc(cs)Slot(py)description":{"name":"description","abstract":"<p>Undocumented</p>","parent_name":"Slot"},"Classes/Slot.html#/s:10Spokestack4SlotC4type5value8rawValueACSS_ypSgSSSgtcfc":{"name":"init(type:value:rawValue:)","abstract":"<p>The initializer for the NLU result slot.</p>","parent_name":"Slot"},"Classes/NLUResult.html#/c:@M@Spokestack@objc(cs)NLUResult(py)utterance":{"name":"utterance","abstract":"<p>The original utterance that was classified.</p>","parent_name":"NLUResult"},"Classes/NLUResult.html#/c:@M@Spokestack@objc(cs)NLUResult(py)intent":{"name":"intent","abstract":"<p>The intent that the utterance was classified as.</p>","parent_name":"NLUResult"},"Classes/NLUResult.html#/c:@M@Spokestack@objc(cs)NLUResult(py)context":{"name":"context","abstract":"<p>Additional context included with the classification results.</p>","parent_name":"NLUResult"},"Classes/NLUResult.html#/c:@M@Spokestack@objc(cs)NLUResult(py)confidence":{"name":"confidence","abstract":"<p>The confidence level of the classification result.</p>","parent_name":"NLUResult"},"Classes/NLUResult.html#/c:@M@Spokestack@objc(cs)NLUResult(py)slots":{"name":"slots","abstract":"<p>The slot values present in the utterance.</p>","parent_name":"NLUResult"},"Classes/NLUResult.html#/c:@M@Spokestack@objc(cs)NLUResult(py)description":{"name":"description","abstract":"<p>Undocumented</p>","parent_name":"NLUResult"},"Classes/NLUResult.html#/s:10Spokestack9NLUResultC9utterance6intent7context10confidence5slotsACSS_SSSDySSypGSfSDySSAA4SlotCGSgtcfc":{"name":"init(utterance:intent:context:confidence:slots:)","abstract":"<p>The initializer for the NLU result.</p>","parent_name":"NLUResult"},"Classes/NLUResult.html#/s:10Spokestack9NLUResultC9utterance6intent7context10confidenceACSS_SSSDySSypGSftcfc":{"name":"init(utterance:intent:context:confidence:)","abstract":"<p>The initializer for the NLU result.</p>","parent_name":"NLUResult"},"Classes/AppleWakewordRecognizer.html#/c:@M@Spokestack@objc(cs)AppleWakewordRecognizer(py)configuration":{"name":"configuration","abstract":"<p>Configuration for the recognizer.</p>","parent_name":"AppleWakewordRecognizer"},"Classes/AppleWakewordRecognizer.html#/c:@M@Spokestack@objc(cs)AppleWakewordRecognizer(py)context":{"name":"context","abstract":"<p>Global state for the speech pipeline.</p>","parent_name":"AppleWakewordRecognizer"},"Classes/AppleWakewordRecognizer.html#/c:@M@Spokestack@objc(cs)AppleWakewordRecognizer(im)init:context:":{"name":"init(_:context:)","abstract":"<p>Initializes a AppleWakewordRecognizer instance.</p>","parent_name":"AppleWakewordRecognizer"},"Classes/AppleWakewordRecognizer.html#/c:@CM@Spokestack@objc(cs)AppleWakewordRecognizer(im)startStreaming":{"name":"startStreaming()","abstract":"<p>Triggered by the speech pipeline, instructing the recognizer to begin streaming and processing audio.</p>","parent_name":"AppleWakewordRecognizer"},"Classes/AppleWakewordRecognizer.html#/c:@CM@Spokestack@objc(cs)AppleWakewordRecognizer(im)stopStreaming":{"name":"stopStreaming()","abstract":"<p>Triggered by the speech pipeline, instructing the recognizer to stop streaming audio and complete processing.</p>","parent_name":"AppleWakewordRecognizer"},"Classes/AppleWakewordRecognizer.html#/c:@CM@Spokestack@objc(cs)AppleWakewordRecognizer(im)process:":{"name":"process(_:)","abstract":"<p>Receives a frame of audio samples for processing. Interface between the <code><a href=\"36f8f5912051ae747ef441d6511ca4cbProtocols/SpeechProcessor.html\">SpeechProcessor</a></code> and <code>AudioController</code> components. Processes audio in an async thread.</p>","parent_name":"AppleWakewordRecognizer"},"Classes/AppleSpeechRecognizer.html#/c:@M@Spokestack@objc(cs)AppleSpeechRecognizer(py)configuration":{"name":"configuration","abstract":"<p>Configuration for the recognizer.</p>","parent_name":"AppleSpeechRecognizer"},"Classes/AppleSpeechRecognizer.html#/c:@M@Spokestack@objc(cs)AppleSpeechRecognizer(py)context":{"name":"context","abstract":"<p>Global state for the speech pipeline.</p>","parent_name":"AppleSpeechRecognizer"},"Classes/AppleSpeechRecognizer.html#/c:@M@Spokestack@objc(cs)AppleSpeechRecognizer(im)init:context:":{"name":"init(_:context:)","abstract":"<p>Initializes a AppleSpeechRecognizer instance.</p>","parent_name":"AppleSpeechRecognizer"},"Classes/AppleSpeechRecognizer.html#/c:@CM@Spokestack@objc(cs)AppleSpeechRecognizer(im)startStreaming":{"name":"startStreaming()","abstract":"<p>Triggered by the speech pipeline, instructing the recognizer to begin streaming and processing audio.</p>","parent_name":"AppleSpeechRecognizer"},"Classes/AppleSpeechRecognizer.html#/c:@CM@Spokestack@objc(cs)AppleSpeechRecognizer(im)stopStreaming":{"name":"stopStreaming()","abstract":"<p>Triggered by the speech pipeline, instructing the recognizer to stop streaming audio and complete processing.</p>","parent_name":"AppleSpeechRecognizer"},"Classes/AppleSpeechRecognizer.html#/c:@CM@Spokestack@objc(cs)AppleSpeechRecognizer(im)process:":{"name":"process(_:)","abstract":"<p>Processes an audio frame, recognizing speech.</p>","parent_name":"AppleSpeechRecognizer"},"Classes/AppleSpeechRecognizer.html":{"name":"AppleSpeechRecognizer","abstract":"<p>This pipeline component uses the Apple <code>SFSpeech</code> API to stream audio samples for speech recognition.</p>"},"Classes/AppleWakewordRecognizer.html":{"name":"AppleWakewordRecognizer","abstract":"<p>This pipeline component uses the Apple <code>SFSpeech</code> API to stream audio samples for wakeword recognition.</p>"},"Classes/NLUResult.html":{"name":"NLUResult","abstract":"<p>A simple data class that represents the result of an utterance classification.</p>"},"Classes/Slot.html":{"name":"Slot","abstract":"<p>A slot extracted during intent classification.</p>"},"Classes/NLUTensorflow.html":{"name":"NLUTensorflow","abstract":"<p>This is the client entry point for the Spokestack BERT NLU implementation. This class provides a classification interface for deriving intents and slots from a natural language utterance. When initialized, the TTS system communicates with the client via either a delegate that receive events or the publisher-subscriber pattern.</p>"},"Classes/SpeechConfiguration.html":{"name":"SpeechConfiguration","abstract":"<p>Configuration properties for Spokestack modules.</p>"},"Classes/SpeechContext.html":{"name":"SpeechContext","abstract":"<p>This class maintains global state for the speech pipeline, allowing pipeline components to communicate information among themselves and event handlers.</p>"},"Classes/SpeechPipeline.html":{"name":"SpeechPipeline","abstract":"<p>This is the primary client entry point to the Spokestack voice input system. It dynamically binds to configured components that implement the pipeline interfaces for reading audio frames and performing speech recognition tasks.</p>"},"Classes/SpeechPipelineBuilder.html":{"name":"SpeechPipelineBuilder","abstract":"<p>Convenience initializer for building a <code><a href=\"36f8f5912051ae747ef441d6511ca4cbClasses/SpeechPipeline.html\">SpeechPipeline</a></code> instance using a pre-configured profile. A pipeline profile encapsulates a series of configuration values tuned for a specific task.</p>"},"Classes/Spokestack.html":{"name":"Spokestack","abstract":"<p>This class combines all Spokestack modules into a single component to provide a unified interface to the library&rsquo;s ASR, NLU, and TTS features. Like the individual modules, it is configurable using a fluent builder pattern, but it provides a default configuration; only a few parameters are required from the calling application, and those only for specific features noted in the documentation for the builder&rsquo;s methods.</p>"},"Classes/SpokestackBuilder.html":{"name":"SpokestackBuilder","abstract":"<p>Fluent builder interface for configuring Spokestack.</p>"},"Classes/SpokestackSpeechRecognizer.html":{"name":"SpokestackSpeechRecognizer","abstract":"<p>This pipeline component streams audio frames to Spokestack&rsquo;s cloud-based ASR for speech recognition.</p>"},"Classes/TFLiteKeywordRecognizer.html":{"name":"TFLiteKeywordRecognizer","abstract":"<p>Undocumented</p>"},"Classes/TFLiteWakewordRecognizer.html":{"name":"TFLiteWakewordRecognizer","abstract":"<p>This pipeline component streams audio samples and uses a TensorFlow Lite binary classifier to detect keyword phrases to process for wakeword recognition. Once a wakeword phrase is detected, the speech pipeline is activated.</p>"},"Classes/TextToSpeech.html":{"name":"TextToSpeech","abstract":"<p>This is the client entry point for the Spokestack Text to Speech (TTS) system. It provides the capability to synthesize textual input, and speak back the synthesis as audio system output. The synthesis and speech occur on asynchronous blocks so as to not block the client while it performs network and audio system activities.</p>"},"Classes/TextToSpeechInput.html":{"name":"TextToSpeechInput","abstract":"<p>Input parameters for speech synthesis. Parameters are considered transient and may change each time <code>synthesize</code> is called.</p>"},"Classes/TextToSpeechResult.html":{"name":"TextToSpeechResult","abstract":"<p>Result of the <code>TextToSpeech.synthesize</code> request.</p>"},"Classes/VADTrigger.html":{"name":"VADTrigger","abstract":"<p>Undocumented</p>"},"Classes/WebRTCVAD.html":{"name":"WebRTCVAD","abstract":"<p>Swift wrapper for WebRTC&rsquo;s voice activity detector.</p>"},"Classes.html":{"name":"Classes","abstract":"<p>The following classes are available globally.</p>"},"Enums.html":{"name":"Enumerations","abstract":"<p>The following enumerations are available globally.</p>"},"Extensions.html":{"name":"Extensions","abstract":"<p>The following extensions are available globally.</p>"},"Protocols.html":{"name":"Protocols","abstract":"<p>The following protocols are available globally.</p>"},"Structs.html":{"name":"Structures","abstract":"<p>The following structures are available globally.</p>"}}